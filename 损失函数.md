# 损失函数
在训练机器学习模型时，我们经常会听到‘损失函数’（Loss Function）这个词，比如线性回归用‘均方误差’，逻辑回归用‘交叉熵’。我们为什么要选择这些特定的函数？

本文将深入探讨这些问题，揭示从最大似然估计到最大后验估计的理论框架，是如何自然而然地推导出这些我们耳熟能详的损失函数与正则化项的。

在深入讨论某一个单独的函数之前，我们先来了解一下它的分类。
## 一、分类
*   **损失函数**
    *   **最大似然估计 (MLE)** - _频率派视角_
        *   **均方误差MSE**：当假设似然函数为高斯分布时导出，等价于**最小二乘法**。
        *   **交叉熵损失**：当假设似然函数为伯努利/多项式分布时导出。
    *   **最大后验估计 (MAP)** - _贝叶斯派视角_
        *   **MAP = MLE + 正则化**
        *   **L2正则化**：当参数的**先验**为**高斯分布**时导出。
        *   **L1正则化**：当参数的**先验**为**拉普拉斯分布**时导出。

主要分为**最大似然估计**和**最大后验估计**两派，我们学过的**均方误差MSE**、**交叉熵损失**、**L1 L2正则化**都可以归为里面的一类。
## 二、最大似然估计
### **2.1先来了解一些概念**
- **1. 似然（Likelihood） vs. 概率（Probability）**
这是一个关键的区别，但很容易混淆。
	*   **概率**：是在**参数已知**的情况下，预测某个**结果**发生的可能性。
	>   例如：给定一个均匀的硬币（参数 $p=0.5$），抛10次出现7正3反的**概率**是多少？
	*   **似然**：是在**结果已知**的情况下，反推某个**参数**的可能性。
    >   例如：已经观测到7正3反（结果已知），硬币是均匀的（参数 $p=0.5$）这个说法的**似然**程度有多大？
    
    > 值得注意的是，从数学表达式上看，似然函数 $L(\theta | x)$ 和概率 $P(x | \theta)$ 是相等的。它们真正的区别在于看待问题的角度：概率是固定参数 $\theta$ 讨论数据 $x$ 的分布，而似然则是固定数据 $x$ 讨论参数 $\theta$ 的可能性。函数本身不变，变量和常量的角色发生了互换。
    
### **2.2 什么是最大似然估计 (Maximum Likelihood Estimation)？**

最大似然估计的核心思想是：**何种参数，最可能产生我们观测到的这批数据？**

我们选择的参数，应该是能让现有数据出现的 **似然（Likelihood）**最大的那个。
 换言之，我们选择的参数应该让已发生的事件看起来最“理所当然”。

对于独立同分布的样本 $X = \{x_1, x_2, ..., x_n\}$，其似然函数为：
$$L(\theta | X) = P(X|\theta) = \prod_{i=1}^{n} P(x_i|\theta)$$
>要取最大似然函数，也就是把 $P(X|\theta)$ ，取到最大值。
>即在$\theta$条件下把产生观测到的这批数据的概率取到最大。

由于对似然函数直接求导和优化通常很复杂（尤其是连乘形式），我们通常将其转换为**对数似然函数**。最大化似然函数等价于最大化对数似然（因为对数在低数大于1的时候时单调递增的），而最大化对数似然则等价于**最小化负对数似然**（就是增函数取负变成减函数）。
>最小化负对数似然
>$$ \theta_{ML} = \arg \min_{\theta} \sum_{i=1}^{m} -\log p_{model}(x^{(i)} | \theta) $$

### **2.3 最大似然估计和均方误差有什么关系？**
先说结论：**在线性回归模型中，假设误差服从均值为0的正态分布，那么通过最小化均方误差来求解模型参数，与通过最大似然估计来求解模型参数是等价的。**
>**均方误差是什么？**
>均方误差是衡量预测值与真实值之间差异的一种常用指标。 它计算的是预测误差平方的平均值。 对于一组真实值为 $y = \{y_1, y_2, ..., y_n\}$，预测值为 $\hat{y} = \{\hat{y}_1, \hat{y}_2, ..., \hat{y}_n\}$ 的样本，其均方误差定义为：
$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$
MSE的值越小，说明模型的预测越精确。

最大似然估计和均方误差之间的关系，在线性回归模型中得到了完美的诠释。 假设一个线性回归模型：
$$y = f(x; \theta) + \epsilon$$
其中，$y$ 是因变量，$x$ 是自变量，$\theta$ 是模型参数，$\epsilon$ 是误差项。

**关键假设：误差服从正态分布**
为了将最大似然估计与均方误差联系起来，我们需要一个关键的假设：**误差项 $\epsilon$ 服从均值为0，方差为 $\sigma^2$ 的正态分布（高斯分布）**。 这个假设在很多实际应用中是合理的，因为根据中心极限定理，大量独立的随机因素叠加产生的误差往往近似于正态分布。

基于这个假设，对于给定的输入 $x_i$，其对应的输出 $y_i$ 也服从正态分布：
$$y_i \sim N(f(x_i; \theta), \sigma^2)$$
>在$x_i$确定之后，把 $f(x_i; \theta)$看成常数，因为$y = f(x; \theta) + \epsilon$，又因为**误差项 $\epsilon$ 服从均值为0，方差为 $\sigma^2$ 的正态分布**，所以$y_i$ 服从均值为$f(x_i; \theta)$，方差为 $\sigma^2$ （加减常数不改变方差）的正态分布。

其概率密度函数为：
$$P(y_i | x_i; \theta, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - f(x_i; \theta))^2}{2\sigma^2}\right)$$

接下来，我们就可以构建似然函数了。
1.  **构建似然函数**：
    对于整个数据集，假设样本之间相互独立，其似然函数为所有样本概率密度函数的乘积：
    $$L(\theta, \sigma^2 | Y, X) = \prod_{i=1}^{n} P(y_i | x_i; \theta, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - f(x_i; \theta))^2}{2\sigma^2}\right)$$
上面我们知道，我们通常将似然函数转换为**对数似然函数**，于是得到：
    $$\ln L(\theta, \sigma^2 | Y, X) = \sum_{i=1}^{n} \ln \left( \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - f(x_i; \theta))^2}{2\sigma^2}\right) \right)$$
    $$\ln L(\theta, \sigma^2 | Y, X) = \sum_{i=1}^{n} \left( -\frac{1}{2}\ln(2\pi\sigma^2) - \frac{(y_i - f(x_i; \theta))^2}{2\sigma^2} \right)$$
    $$\ln L(\theta, \sigma^2 | Y, X) = -\frac{n}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - f(x_i; \theta))^2$$
>这里log的底数可以取任意大于1的值，都是单调递增的。为了方便，我们不妨取e。

我们的目标是找到使对数似然函数最大化的参数 $\theta$。观察上式，第一项 $-\frac{n}{2}\ln(2\pi\sigma^2)$ 是一个与 $\theta$ 无关的常数。 因此，**最大化对数似然函数就等价于最小化**下式：
    $$\sum_{i=1}^{n} (y_i - f(x_i; \theta))^2$$

我们惊讶地发现：最小化上式等价于最小化均方误差	！
    $$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$
    其中 $\hat{y}_i = f(x_i; \theta)$。
    
因此，我们得出了一个重要的结论：**在线性回归模型中，假设误差服从均值为0的正态分布，那么通过最小化均方误差来求解模型参数，与通过最大似然估计来求解模型参数是等价的。** 这也解释了为什么在线性回归中，我们通常使用最小二乘法（其目标函数就是均方误差）来拟合模型。

### **2.4 最大似然估计和交叉熵有什么关系？**
先上结论： **对模型进行最大似然估计，就等同于最小化 模型预测的概率分布 与 训练数据的经验分布 之间的交叉熵**。
很多名词，我们来一个个解释：
>**模型预测的概率分布**，就是我们上面提到的 模型预测“概率密度函数”。

>**训练数据的经验分布**:我们可以用这个有限的数据集来定义一个**经验分布 (Empirical Distribution)** $\hat{p}_{data}(x)$。
对于任何一个值 $x$，它在数据集 $D$ 中出现的频率就是它的经验概率。如果数据集中有 $m$ 个样本，其中值 $x$ 出现了 $count(x)$ 次，那么：
$$ \hat{p}_{data}(x) = \frac{\text{count}(x)}{m} $$
这个经验分布 $\hat{p}_{data}$ 是我们基于观测数据对真实数据分布 $p_{data}$ 的一个近似。

>交叉熵（Cross-Entropy）是用来衡量两个概率分布之间差异的指标。对于离散分布 $P$ 和 $Q$，其交叉熵定义为：
$$ H(P, Q) = - \sum_{x} P(x) \log Q(x) $$

了解了这些，我们终于可以开始推导了。
最大化对数似然函数与最大化**平均对数似然函数**（就是多除了一个m）是等价的，因为 $1/m$ 是一个正的常数，不影响 $\arg \max$ 的取值。
$$ \theta_{ML} = \arg \max_{\theta} \frac{1}{m} \sum_{i=1}^{m} \log p_{model}(x_i | \theta) $$
接下来，我们可以对求和项进行重新组合。我们不再对数据集中的每一个样本 $x_i$ 进行求和，而是对所有可能的数据值 $x$ 进行求和，并用它们在数据集中出现的次数进行加权：
$$ \frac{1}{m} \sum_{i=1}^{m} \log p_{model}(x_i | \theta) = \frac{1}{m} \sum_{x} \text{count}(x) \log p_{model}(x | \theta) $$
现在，把 $\frac{1}{m}$ 乘到求和符号里面：
$$ = \sum_{x} \frac{\text{count}(x)}{m} \log p_{model}(x | \theta) $$
我们发现，$\frac{\text{count}(x)}{m}$ 正是经验分布 $\hat{p}_{data}(x)$ 的定义！所以上式可以写为：
$$ = \sum_{x} \hat{p}_{data}(x) \log p_{model}(x | \theta) $$
因此，我们证明了：
$$ \arg \max_{\theta} \sum_{i=1}^{m} \log p_{model}(x_i | \theta) \quad \Longleftrightarrow \quad \arg \max_{\theta} \sum_{x} \hat{p}_{data}(x) \log p_{model}(x | \theta) $$
两者是完全等价的。
因此，我们得到结论： **对模型进行最大似然估计，就等同于最小化 模型预测的概率分布 与 训练数据的经验分布 之间的交叉熵**。
>**拓展：为什么选择它作为损失函数？**
在神经网络的训练中，我们依赖**梯度下降**来更新参数。损失函数对模型输出的梯度（导数）至关重要。
当交叉熵损失与**Softmax**激活函数（在多分类任务中是标配）结合时，它会产生一个非常简洁和高效的梯度。有兴趣可以自行查阅。

## 三、最大后验估计
>建议先观看3b1b的关于贝叶斯公式的视频，以便理解。
>https://www.youtube.com/watch?v=HZGCoVF3YvM
### **3.1 最大后验估计是什么**

贝叶斯派的核心不同在于：它认为参数 $\theta$ 不是一个固定的常量，而是一个**随机变量**。在我们观测数据之前，我们对 $\theta$ 就有一个**先验信念（Prior Belief）**，这个信念由一个**先验分布 $f(\theta)$** 来描述。

 **目标**: $\arg\max_{\theta} P(Y, X | θ) f(θ)$
* 贝叶斯派的目标是最大化**后验概率（Posterior Probability）** $P(\theta | Y, X)$。根据贝叶斯定理：
        $P(\theta | Y, X) = \frac{P(Y, X | \theta) \cdot f(\theta)}{P(Y, X)}$
        (后验概率 = (似然 × 先验) / 证据)
 *在最大化这个式子时，分母 $P(Y,X)$ 与 $\theta$ 无关，可以忽略。因此，目标就变成了最大化分子部分：`似然 × 先验`，即 `P(Y, X | θ) f(θ)`。
* **核心思想（最大后验估计, MAP）**：结合我们对数据的观测（似然）和我们对参数的先验信念（先验），找到最可信的参数 $\theta$。

 **推导过程**:
*   与MLE类似，我们对目标函数 $P(Y, X | θ) f(θ)$ 取对数，然后取负，将最大化问题转为最小化问题。
原式：$argmin_θ [ -log(P(Y, X | θ) f(θ)) ]$
$\Longrightarrow$ $argmin_θ [ -log(P(Y, X | θ)) - log(f(θ)) ]$
*   我们已经从左侧知道，第一项 $-log(P(Y, X | θ))$ 对应着最小二乘法的损失项 $\sum (y_i - f(x_i; \theta))^2$。
**结论**: `argmin_θ [ Σ(y_i - f(x_i; θ))^2 - log(f(θ)) ]`
*   这个最终的损失函数由两部分构成：
	*   **第一部分**：和MLE一样的最小二乘损失项。
	*   **第二部分**: `-log(f(θ))`，这一项来自于我们对参数的**先验分布**。它在机器学习中扮演着**正则化项（Regularization Term）**的角色。
>   例如，如果我们假设参数的先验分布 $f(\theta)$ 是一个**高斯分布**，那么 `-log(f(θ))` 就会变成一个 **L2正则化项**（Ridge回归）。如果假设是**拉普拉斯分布**，它就会变成一个 **L1正则化项**（LASSO回归）。
但是，为什么？？！！
---
懒得写了，贴一段Gemini的回答：

 **1. 高斯先验 (Gaussian Prior) → L2 正则化 (Ridge)**

**第一步：写出高斯先验的概率密度函数 (PDF)**

我们假设参数 $\theta$ 的每个分量 $\theta_j$ 都独立地服从一个以0为均值、$\sigma^2$为方差的高斯分布（正态分布）。这是一个非常常见的“模型参数不应该太大”的先验信念。

单个参数 $\theta_j$ 的概率密度函数为：
$f(\theta_j) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\theta_j^2}{2\sigma^2}\right)$

由于我们假设所有参数分量都是独立的，整个参数向量 $\theta$ 的先验概率就是所有分量概率的连乘：
$f(\theta) = \prod_{j=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\theta_j^2}{2\sigma^2}\right) = \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp\left(-\frac{1}{2\sigma^2}\sum_{j=1}^{n}\theta_j^2\right)$

**第二步：计算 $-\log(f(\theta))$**

现在我们对这个式子取负对数。使用对数法则 $\log(ab) = \log(a) + \log(b)$ 和 $\log(e^x) = x$：
$-\log(f(\theta)) = -\log\left[ \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp\left(-\frac{1}{2\sigma^2}\sum_{j=1}^{n}\theta_j^2\right) \right]$
$= -\left[ \log\left(\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n\right) + \log\left(\exp\left(-\frac{1}{2\sigma^2}\sum_{j=1}^{n}\theta_j^2\right)\right) \right]$
$= -\left[ n\log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) - \frac{1}{2\sigma^2}\sum_{j=1}^{n}\theta_j^2 \right]$
$= \frac{1}{2\sigma^2}\sum_{j=1}^{n}\theta_j^2 - n\log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)$

**第三步：放入优化目标中**

我们的目标是最小化 `Original_Loss - log(f(θ))`。将上式代入：
$\text{argmin}_{\theta} \left[ \text{Original\_Loss} + \frac{1}{2\sigma^2}\sum_{j=1}^{n}\theta_j^2 - n\log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) \right]$

在优化过程中，我们只关心与参数 $\theta$ 相关的项。后面那项 $- n\log(...)$ 是一个不依赖于 $\theta$ 的常数，对寻找最优 $\theta$ 的位置没有影响，因此可以舍去。

于是优化目标简化为：
$\text{argmin}_{\theta} \left[ \text{Original\_Loss} + \frac{1}{2\sigma^2}\sum_{j=1}^{n}\theta_j^2 \right]$

如果我们令正则化系数 $\lambda = \frac{1}{2\sigma^2}$，这就变成了我们非常熟悉的形式：
$\text{argmin}_{\theta} \left[ \text{Original\_Loss} + \lambda \sum_{j=1}^{n}\theta_j^2 \right]$

这里的 $\sum \theta_j^2$ 就是**L2正则化项**。

---

 **2. 拉普拉斯先验 (Laplace Prior) → L1 正则化 (LASSO)**

**第一步：写出拉普拉斯先验的概率密度函数**

现在，我们换一个先验信念：我们认为参数 $\theta$ 应该是稀疏的，即很多 $\theta_j$ 应该正好是0。这个信念可以用拉普拉斯分布来很好地描述，因为它在0点有一个尖峰。

单个参数 $\theta_j$ 服从均值为0、尺度参数为$b$的拉普拉斯分布：
$f(\theta_j) = \frac{1}{2b} \exp\left(-\frac{|\theta_j|}{b}\right)$

同样，假设独立，整个参数向量 $\theta$ 的先验概率为：
$f(\theta) = \prod_{j=1}^{n} \frac{1}{2b} \exp\left(-\frac{|\theta_j|}{b}\right) = \left(\frac{1}{2b}\right)^n \exp\left(-\frac{1}{b}\sum_{j=1}^{n}|\theta_j|\right)$

**第二步：计算 $-\log(f(\theta))$**

同样取负对数：
$-\log(f(\theta)) = -\log\left[ \left(\frac{1}{2b}\right)^n \exp\left(-\frac{1}{b}\sum_{j=1}^{n}|\theta_j|\right) \right]$
$= -\left[ n\log\left(\frac{1}{2b}\right) - \frac{1}{b}\sum_{j=1}^{n}|\theta_j| \right]$
$= \frac{1}{b}\sum_{j=1}^{n}|\theta_j| - n\log\left(\frac{1}{2b}\right)$

**第三步：放入优化目标中**

代入并舍去与 $\theta$ 无关的常数项，优化目标变为：
$\text{argmin}_{\theta} \left[ \text{Original\_Loss} + \frac{1}{b}\sum_{j=1}^{n}|\theta_j| \right]$

如果我们令正则化系数 $\lambda = \frac{1}{b}$，就得到了LASSO回归的损失函数：
$\text{argmin}_{\theta} \left[ \text{Original\_Loss} + \lambda \sum_{j=1}^{n}|\theta_j| \right]$

这里的 $\sum |\theta_j|$ 就是**L1正则化项**。

 **直观总结**

| 特性 | 高斯先验 (L2) | 拉普拉斯先验 (L1) |
| :--- | :--- | :--- |
| **先验分布形状** | 钟形曲线，平滑 | 在0点有尖峰，"Pointy" |
| **-log(先验) 惩罚项形状** | 抛物线 ($y=x^2$) | V形绝对值 ($y=|x|$) |
| **对权重的处理** | 倾向于让权重值变得很小，但**不为0**。 | 倾向于让不重要的权重值**直接变为0**，实现稀疏性。 |
| **机器学习应用** | Ridge 回归，权重衰减 (Weight Decay) | LASSO 回归，用于特征选择 |

---
## 四、总结
至此，我们终于把损失函数讲完了。

我们从**最大似然估计 (MLE)**出发，到**最大后验估计 (MAP)**，两者关系不过就是**MAP损失 = MLE损失 + 参数先验的负对数**。

而最大后验估计和L1、L2的关系：
 **最大后验估计 + 参数服从拉普拉斯分布的先验 $\implies$ L1 正则化**
 **最大后验估计 + 参数服从高斯分布的先验 $\implies$ L2 正则化**
